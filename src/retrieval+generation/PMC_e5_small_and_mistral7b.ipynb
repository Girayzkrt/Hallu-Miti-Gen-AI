{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbe40b9",
   "metadata": {},
   "source": [
    "### This Notebook is for Retrieving data with e5-small-v2 embedding model and generate answers with Mistral7b LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b42cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qdrant_models\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = \"intfloat/e5-small-v2\"\n",
    "QDRANT_HOST = \"localhost\"\n",
    "QDRANT_PORT = 6333\n",
    "COLLECTION_NAME = \"pmc_chunked_title_abstract\"\n",
    "EMBED_DIM = 384\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc043da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant connection ok\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text):\n",
    "    inp = f\"passage: {text.strip()}\"\n",
    "    encoded = tokenizer(inp, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        encoded = {k: v.cuda() for k, v in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**encoded)\n",
    "    return out.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)\n",
    "print(\"Qdrant connection ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd29e767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc ID 1197115 — cosine similarity: 0.0912\n",
      "Score: 0.9088169\n",
      "Doc ID 983637 — cosine similarity: 0.1108\n",
      "Score: 0.88918495\n",
      "Doc ID 192456 — cosine similarity: 0.1127\n",
      "Score: 0.8873118\n",
      "Retrieved context: Title: Patient-Controlled Therapy with Intravenous Oxycodone in Breathlessness due to Advanced Cancer: A Case Report\n",
      "Abstract: Dyspnoea is a debilitating symptom in medicine, especially in palliative care. Opioids are the pharmacological agents of choice in the treatment of dyspnoea in palliative medicine. Morphine is the best-studied opioid, and recent literature on oxycodone is encouraging. In refractory cases, opioid infusion and palliative sedation may have to be used. We present a case that used oxycodone in a patient-controlled device specifically for dyspnoea and its effects in relieving dyspnoea in a fast and timely manner. This helped in meeting the demands of the patient and relieving suffering rapidly with less sedation. This case report is unique in the use of an oxycodone patient-controlled device specifically for dyspnoea.\n",
      "\n",
      "Title: Real‐world opioid prescription to patients with serious, non‐malignant, respiratory illnesses and chronic breathlessness\n",
      "Abstract: Chronic breathlessness is a disabling symptom that is often under‐recognised and challenging to treat despite optimal disease‐directed therapy. Low‐dose, oral opioids are recommended to relieve breathlessness, but little is known regarding long‐term opioid prescription in this setting. To investigate the long‐term efficacy of, and side‐effects from, opioids prescribed for chronic breathlessness to patients with advanced, non‐malignant, respiratory diseases. A prospective cohort study of all patients managed by the advanced lung disease service, an integrated respiratory and palliative care service, at the Royal Melbourne Hospital from 1 April 2013 to 3 March 2020. One hundred and nine patients were prescribed opioids for chronic breathlessness. The median length of opioid use was 9.8 (interquartile range (IQR) = 2.8–19.8) months. The most commonly prescribed initial regimen was an immediate‐release preparation (i.e. Ordine) used as required (37; 33.9%). For long‐term treatment, the most frequently prescribed regimen included an extended‐release preparation with an as needed immediate‐release (37; 33.9%). The median dose prescribed was 12 (IQR = 8–28) mg oral morphine equivalents/day. Seventy‐one (65.1%) patients reported a subjective improvement in breathlessness. There was no significant change in the mean modified Medical Research Council dyspnoea score (P = 0.807) or lung function measurements (P = 0.086–0.727). There was no association between mortality and the median duration of opioid use (P = 0.201) or dose consumed (P = 0.130). No major adverse events were reported. Within this integrated respiratory and palliative care service, patients with severe, non‐malignant respiratory diseases safely used long‐term, low‐dose opioids for breathlessness with subjective benefits reported and no serious adverse events.\n",
      "\n",
      "Title: Can variability in the effect of opioids on refractory breathlessness be explained by genetic factors?\n",
      "Abstract: Opioids modulate the perception of breathlessness with a considerable variation in response, with poor correlation between the required opioid dose and symptom severity. The objective of this hypothesis-generating, secondary analysis was to identify candidate single nucleotide polymorphisms (SNP) from those associated with opioid receptors, signalling or pain modulation to identify any related to intensity of breathlessness while on opioids. This can help to inform prospective studies and potentially lead to better tailoring of opioid therapy for refractory breathlessness. 17 hospice/palliative care services (tertiary services) in 11 European countries. 2294 people over 18 years of age on regular opioids for pain related to cancer or its treatment. The relationship between morphine dose, breathlessness intensity (European Organisation for Research and Treatment of Cancer Core Quality of Life Questionnaire; EORTCQLQC30 question 8) and 112 candidate SNPs from 25 genes (n=588). The same measures for people on oxycodone (n=402) or fentanyl (n=429). SNPs not in Hardy-Weinberg equilibrium or with allele frequencies (<5%) were removed. Univariate associations between each SNP and breathlessness intensity were determined with Benjamini-Hochberg false discovery rate set at 20%. Multivariable ordinal logistic regression, clustering over country and adjusting for available confounders, was conducted with remaining SNPs. For univariate morphine associations, 1 variant on the 5-hydroxytryptamine type 3B (HTR3B) gene, and 4 on the β-2-arrestin gene (ARRB2) were associated with more intense breathlessness. 1 SNP remained significant in the multivariable model: people with rs7103572 SNP (HTR3B gene; present in 8.4% of the population) were three times more likely to have more intense breathlessness (OR 2.86; 95% CIs 1.46 to 5.62; p=0.002). No associations were seen with fentanyl nor with oxycodone. This large, exploratory study identified 1 biologically plausible SNP that warrants further study in the response of breathlessness to morphine therapy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krtgi\\AppData\\Local\\Temp\\ipykernel_14476\\302362373.py:4: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n"
     ]
    }
   ],
   "source": [
    "query = \"Patient-Controlled Therapy of Breathlessness in Palliative Care: A New Therapeutic Concept for Opioid Administration?\"\n",
    "query_emb = get_embedding(query)\n",
    "\n",
    "hits = client.search(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query_vector=query_emb.tolist(),\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "context_chunks = []\n",
    "for hit in hits:\n",
    "    cos_sim = 1.0 - hit.score\n",
    "    print(f\"Doc ID {hit.id} — cosine similarity: {cos_sim:.4f}\")\n",
    "    print(f\"Score: {hit.score}\")\n",
    "    p = hit.payload\n",
    "    chunk = []\n",
    "    if p.get(\"title\"):\n",
    "        chunk.append(f\"Title: {p['title']}\")\n",
    "        if p.get(\"abstract\"):\n",
    "            chunk.append(f\"Abstract: {p['abstract']}\")\n",
    "\n",
    "    \n",
    "    context_chunks.append(\"\\n\".join(chunk))\n",
    "\n",
    "retrieved_context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "\n",
    "\n",
    "retrieved_context = \"\\n\\n\".join(context_chunks)\n",
    "print(\"Retrieved context:\", retrieved_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736a4771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c61901701564c21bad6cbe49bb4594d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = (\n",
    "    \"You are PubMed QA-Bot, a cautious biomedical expert.\\n\"\n",
    "    \"• Read the CONTEXT exactly as given—do not invent facts.\\n\"\n",
    "    \"• If the answer is present, quote or paraphrase only material you can trace to a specific sentence in the CONTEXT.\\n\"\n",
    "    \"• If the answer is partly or wholly absent, reply exactly with: 'INSUFFICIENT CONTEXT'.\\n\"\n",
    "    \"• Use concise scientific language and original terminology from the CONTEXT.\\n\"\n",
    "    \"• Write the ANSWER in ≤ 40 words. Do NOT include citations in the ANSWER line.\\n\"\n",
    "    \"• After the ANSWER, output a line that starts with 'EVIDENCE:' followed by the sentence IDs you used.\\n\"\n",
    ")\n",
    "\n",
    "demo_user = (\n",
    "    \"QUESTION:\\nWhat is the main benefit of opioid patient-controlled therapy (PCT) in palliative care patients?\\n\\n\"\n",
    "    \"CONTEXT:\\n\"\n",
    "    \"[1] Opioid PCT allows patients to self-administer small boluses...\\n\"\n",
    ")\n",
    "demo_assistant = (\n",
    "    \"ANSWER: Opioid PCT safely reduces refractory breathlessness in palliative patients while giving them direct symptom control.\\n\"\n",
    "    \"EVIDENCE: 1-2\"\n",
    ")\n",
    "\n",
    "user_message = (\n",
    "    f\"QUESTION:\\n{query}\\n\\n\"\n",
    "    f\"CONTEXT:\\n{retrieved_context}\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # {\"role\": \"user\", \"content\": demo_user},\n",
    "    # {\"role\": \"assistant\", \"content\": demo_assistant},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a84a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1515\n",
      "Tokenization complete\n",
      "Prompt length: 5752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krtgi\\anaconda3\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Patient-Controlled Therapy of Breathlessness in Palliative Care: A New Therapeutic Concept for Opioid Administration?\n",
      "Answer: ANSWER:\n",
      "The case report describes the use of a patient-controlled oxycodone device for dyspnoea relief in palliative care, while the study investigates long-term opioid prescription for chronic breathlessness in non-malignant respiratory diseases. No specific information is provided regarding genetic factors influencing opioid response for breathlessness in these texts.\n",
      "\n",
      "EVIDENCE:\n",
      "1. Case Report: Abstract and specific sentence: \"We present a case that used oxycodone in a patient-controlled device specifically for dyspnoea and its effects in relieving dyspnoea in a fast and timely manner.\"\n",
      "2. Study: Abstract and specific sentences: \"To investigate the long-term efficacy of, and side-effects from, opioids prescribed for chronic breathlessness to patients with advanced, non-malignant, respiratory diseases.\" \"No associations were seen with fentanyl nor with oxycodone.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"Token count:\", len(tokenizer(prompt)[\"input_ids\"]))\n",
    "\n",
    "print(\"Tokenization complete\")\n",
    "\n",
    "print(\"Prompt length:\", len(prompt))\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=300,\n",
    "    do_sample=False\n",
    ")\n",
    "generated = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "answer = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
