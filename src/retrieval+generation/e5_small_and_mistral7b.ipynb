{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbe40b9",
   "metadata": {},
   "source": [
    "### This Notebook is for Retrieving data with e5-small-v2 embedding model and generate answers with Mistral7b LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b42cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qdrant_models\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_NAME = \"intfloat/e5-small-v2\"\n",
    "QDRANT_HOST = \"localhost\"\n",
    "QDRANT_PORT = 6333\n",
    "COLLECTION_NAME = \"pqa_labeled\"\n",
    "EMBED_DIM = 384\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc043da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant connection ok\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text):\n",
    "    inp = f\"passage: {text.strip()}\"\n",
    "    encoded = tokenizer(inp, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        encoded = {k: v.cuda() for k, v in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**encoded)\n",
    "    return out.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)\n",
    "print(\"Qdrant connection ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29e767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: Knowledge: ['Apparent life-threatening events in infants are a difficult and frequent problem in pediatric practice. The prognosis is uncertain because of risk of sudden infant death syndrome.'\n",
      " 'Eight infants aged 2 to 15 months were admitted during a period of 6 years; they suffered from similar maladies in the bath: on immersion, they became pale, hypotonic, still and unreactive; recovery took a few seconds after withdrawal from the bath and stimulation. Two diagnoses were initially considered: seizure or gastroesophageal reflux but this was doubtful. The hypothesis of an equivalent of aquagenic urticaria was then considered; as for patients with this disease, each infant\\'s family contained members suffering from dermographism, maladies or eruption after exposure to water or sun. All six infants had dermographism. We found an increase in blood histamine levels after a trial bath in the two infants tested. The evolution of these \"aquagenic maladies\" was favourable after a few weeks without baths. After a 2-7 year follow-up, three out of seven infants continue to suffer from troubles associated with sun or water.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krtgi\\AppData\\Local\\Temp\\ipykernel_30920\\1500990564.py:5: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n"
     ]
    }
   ],
   "source": [
    "query = \"Syncope during bathing in infants, a pediatric form of water-induced urticaria?\"\n",
    "query_emb = get_embedding(query)\n",
    "\n",
    "hits = client.search(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query_vector=query_emb.tolist(),\n",
    "    limit=1\n",
    ")\n",
    "\n",
    "context_chunks = []\n",
    "for hit in hits:\n",
    "    p = hit.payload\n",
    "    chunk = []\n",
    "    if p.get(\"Knowledge\"):\n",
    "        chunk.append(f\"Knowledge: {p['Knowledge']}\")\n",
    "\n",
    "    \n",
    "    context_chunks.append(\"\\n\".join(chunk))\n",
    "\n",
    "retrieved_context = \"\\n\\n---\\n\\n\".join(context_chunks)\n",
    "\n",
    "\n",
    "retrieved_context = \"\\n\\n\".join(context_chunks)\n",
    "print(\"Retrieved context:\", retrieved_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736a4771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343eaa9967164203ba83f8a4ebe7461c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c84ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = (\n",
    "    \"You are a medical assistant answering user queries concisely and accurately. \"\n",
    "    \"Always rely strictly on the retrieved context provided to answer medical questions. \"\n",
    "    \"Provide your answers in 2-3 clear, informative sentences, avoiding speculation. Do NOT use bullet points or lists.\"\n",
    "    \"Example:\\n\"\n",
    "    \"Question: What are the outcomes of two different techniques for cataract surgery?\\n\"\n",
    "    \"Answer: Both techniques for cataract surgery are effective, and most patients recover well. There are no major differences in safety or vision outcomes between them. The choice usually depends on the surgeon’s experience and the patient’s unique needs. In general, both approaches are considered safe and successful. Your doctor can help choose the best option for you.\\n\\n\"\n",
    "    f\"Retrieved Context:\\n{retrieved_context}\"\n",
    ")\n",
    "\n",
    "messages_with_context = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a84a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 460\n",
      "Tokenization complete\n",
      "Prompt length: 1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krtgi\\anaconda3\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Syncope during bathing in infants, a pediatric form of water-induced urticaria?\n",
      "Answer: Syncope during bathing in infants can be a concerning issue. While it was initially thought to be related to seizures or gastroesophageal reflux, recent research suggests it may be related to aquagenic urticaria. This condition is characterized by hives or welts appearing after exposure to water or sun. The infants in question exhibited symptoms of pallor, hypotonia, and unresponsiveness during bathing, which resolved after removal from the water. Dermographism, a sign of urticaria, was present in all affected infants. An increase in blood histamine levels was observed after a trial bath in two infants. The prognosis for these infants was favorable after avoiding baths for a few weeks. However, some infants continued to experience troubles associated with sun or water even after a long-term follow-up.\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages_with_context, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"Token count:\", len(tokenizer(prompt)[\"input_ids\"]))\n",
    "\n",
    "print(\"Tokenization complete\")\n",
    "\n",
    "print(\"Prompt length:\", len(prompt))\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=300,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "generated = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "answer = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
