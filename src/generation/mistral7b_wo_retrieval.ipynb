{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5ba3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426d25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Syncope during bathing in infants, a pediatric form of water-induced urticaria?\"\n",
    "system_message = (\n",
    "    \"You are a medical assistant answering user queries concisely and accurately. \"\n",
    "    \"Always rely strictly on your knowledge about the topic to answer medical questions. \"\n",
    "    \"Provide your answers in 2-3 clear, informative sentences, avoiding speculation. Do NOT use bullet points or lists.\"\n",
    "    \"Example:\\n\"\n",
    "    \"Question: What are the outcomes of two different techniques for cataract surgery?\\n\"\n",
    "    \"Answer: Both techniques for cataract surgery are effective, and most patients recover well. There are no major differences in safety or vision outcomes between them. The choice usually depends on the surgeon’s experience and the patient’s unique needs. In general, both approaches are considered safe and successful. Your doctor can help choose the best option for you.\\n\\n\"\n",
    ")\n",
    "\n",
    "messages_with_context = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72671462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785840ca7ed1400fb17f8b9cf7a1a215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fe048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 185\n",
      "Tokenization complete\n",
      "Prompt length: 837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krtgi\\anaconda3\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Syncope during bathing in infants, a pediatric form of water-induced urticaria?\n",
      "Answer: Syncope during bathing in infants is not directly related to water-induced urticaria. Urticaria, or hives, is a skin condition characterized by itchy, red, raised areas of skin. Syncope, or fainting, is a loss of consciousness due to insufficient blood flow to the brain. While some infants may experience urticaria during or after bathing, syncope is typically caused by various factors such as dehydration, low blood sugar, or certain medical conditions. If you have concerns about your infant's syncope during bathing, it's best to consult a healthcare professional for proper evaluation and guidance.\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages_with_context, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "print(\"Token count:\", len(tokenizer(prompt)[\"input_ids\"]))\n",
    "\n",
    "print(\"Tokenization complete\")\n",
    "\n",
    "print(\"Prompt length:\", len(prompt))\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=300,\n",
    "    do_sample=False,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "generated = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "answer = tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
